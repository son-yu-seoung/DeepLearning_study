# < 목차 >
+ [강화학습이란](#Reinforce-learning)
+ [강화학습의 종류](#CNN-종류)  
  - [AlexNet](#AlexNet)
  - [LeNet](#LeNet)
  - [ZFNet](#ZFNet)
  - [GoogleNet](#GoogleNet)
  - [U-Net](#U-Net)
  - [ResNet](#ResNet)
+ [Hyperparameter](#Hyperparameter)
+ [Overfitting](#Overfitting)

# Reinforce-learning
머신러닝에는 지도학습, 비지도학습, 강화학습이 있다. 강화학습은 지도, 비지도학습과 성격이 달라 머신러닝에서 따로 분류된다.  
정답이 주어진 것은 아니지만 그저 주어진 데이터에 대해 학습하는 것도 아니기 때문이다.  
강화학습은 "보상"을 통해 학습한다. 보상은 컴퓨터가 선택한 "행동"에 대한 환경의 반응이다.  
이 "보상"은 직접적인 답은 아니지만 컴퓨터에게는 간접적인 정답의 역할을 한다.  

**앞으로 강화학습을 통해 스스로 학습하는 컴퓨터를 에이전트라고 할 것이다.**  
에이전트는 환경에 대해 사전지식이 없는 상태에서 학습을 한다. 에이전트는 자신이 놓인 환경에서  
자신의 상태를 인식한 후 행동하게 된다. 그러면 환경은 에이전트에게 보상을 주고 다음 상태를 알려준다.  
이 보상을 통해 에이전트는 어떤 행동이 좋은 행동인지 간접적으로 알게된다. 
- 보상은 양수로 설정할 수도 있고 경우에 따라서 음수로 설정할 수도 있다.  
- 보상을 음수로 설정한다면 처벌이 된다.  상벌을 적적히 융합할 수 있다면 효과적인 학습이 가능하다.  

### 강화학습은 어떤 문제에 적용할까?
강화학습은 마치 사람처럼 환경과 상호작용하면서 스스로 학습되는 방식이다.  
그러므로 강화학습은 결정을 순차적으로 내려야 하는 문제에 적용한다.
현재 위치에서 행동을 한 번 선택하는 것이 아니라 계속적으로 선택해야 한다.
**하지만** 이렇게 순차적으로 결정을 내리는 문제의 해결책이 강화학습만 있는 것은 아니다.
**다이나믹 프로그래밍, 진화 알고리즘 또한 이러한 문제를 푸는데 적용할 수 있으며 강화학습이 그 한계를 극복할 수 있다.**  

### 순차적 행동 결정 문제 
100명의 학생이 있고 학생들의 수학 실력을 비교해서 수학 성적을 높일 전략을 세우려 한다면 어떻게 학생들의 수학 실력을 비교할 수 있을까?  
가장 간단한 방법은 시험을 통해 수학 실력을 수치화 하는 것이다. 만약 학생들의 수학 실력을 수치화하지 않는다면 수학 점수를 높이기 위한 전략을 세우기 어렵다.  
이와 마찬가지로 에이전트가 학습하고 발전하려면 문제를 수학적으로 표현해야 한다.   
그렇지 않으면 에이전트의 입장에서는 학습을 하거나 최적화하기 어려울 것이기 떄문이다.  

순차적으로 행동을 결정하는 문제를 정의할 때 사용하는 방법이 MDP(Markov Decision Process)이다.  
MDP는 순차적 행동 결정 문제를 수학적으로 정의해서 에이전트가 순차적 행동 결정 문제에 접근할 수 있게 해준다.

### 순차적 행동 결정 문제의 구성 요소
수학적으로 정의된 문제는 다음과 같은 구성을 요소를 가진다. 이 구성 요소들을 MDP라고 부른다.  

**1. 상태(state)**  
에이전트의 상태로서 공학에서 많이 사용하는 개념이다.  
상태의 정의가 중요한데, 에이전트가 상태를 통해 상황을 판단해서 행동을 결정하기에 충분한 정보를 제공해야 한다.  
ex) 탁구를 치는 에이전트라고 생각을 해볼때 탁구공의 위치만 알고 속도를 모른다면 에이전트는 사실상 탁구를 칠 수가 없다.  
에이전트가 탁구를 치는 것을 학습하려면 위치, 속도, 가속도와 같은 정보가 필요하다.

**2. 행동(action)**  
에이전트가 어떠한 상태에서 취할 수 있는 행동으로서 "상", "하", "좌", "우"와 같은 것을 말한다.  
게임에서의 행동이라면 게임기를 통해 줄 수 있는 입력일 것이다.  

**3. 보상(reward)**  
보상은 강화학습을 다른 머신러닝 기법과 다르게 만들어주는 가장 핵심정인 요소이다.  
사실상 에이전트가 학습할 수 있는 유일한 정보가 바로 보상이다. 이 보상이라는 정보를 통해 에이전트는 자신이 했던 행동들을 평가할 수 있고,  
이로 인해 어떤 행동이 좋은 행동인지 알 수 있다.  

앞에서 말했듯이 강화학습의 목푠느 시간에 따라 얻는 보상들의 합을 최대로 하는 정책을 찾는 것이다. 보상은 에이전트에 속하지 않는 환경의 일부이다.  
에이전트는 어떤 상황에서 얼마의 보상이 나오는지 미리 알지 못한다.  

**4. 정책(policy)**  
순차적 행동 결정 문제에서 구해야할 답은 바로 정책이다. 에이전트가 보상을 얻으려면 행동을 해야 하는데  
특정 상태가 아닌 모든 상태에 대해 어떤 행동을 해야할 지 알아야 한다.  이렇게 모든 상태에 대해 에이전트가 어떤 행동을 해야 하는지 정해 놓은 것이 정책이다.  

순차적 행동 결정 문제를 풀었다고 한다면 제일 좋은 정책을 에이전트가 얻었다는 것이다.  
제일 좋은 정책은 "최적 정책"이라고 하면 에이전트는 최적 정책에 따라 행동했을 때 보상의 합을 최대로 받을 수 있다.  

---
여기까지 강화 학습의 문제의 정의에 대해 간단히 살펴봤다.  
강화학습은 문제의 정의를 어떻게 설정하느냐에 따라 학습을 잘하는지가 결정된다. 따라서 적절한 보상을 받으면 학습을 할 수 있게 하는 것이 중요하다.  
그리고 에이전트가 판단하기에 충분한 정보를 얻을 수 있도록 순차적 행동 결정 문제를 정의해야 한다.
