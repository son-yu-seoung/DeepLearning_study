# < 목차 >
+ [강화학습이란](#Reinforce-learning)
+ [강화학습의 종류](#CNN-종류)  
  - [AlexNet](#AlexNet)
  - [LeNet](#LeNet)
  - [ZFNet](#ZFNet)
  - [GoogleNet](#GoogleNet)
  - [U-Net](#U-Net)
  - [ResNet](#ResNet)
+ [Hyperparameter](#Hyperparameter)
+ [Overfitting](#Overfitting)

# Reinforce-learning
머신러닝에는 지도학습, 비지도학습, 강화학습이 있다. 강화학습은 지도, 비지도학습과 성격이 달라 머신러닝에서 따로 분류된다.  
정답이 주어진 것은 아니지만 그저 주어진 데이터에 대해 학습하는 것도 아니기 때문이다.  
강화학습은 "보상"을 통해 학습한다. 보상은 컴퓨터가 선택한 "행동"에 대한 환경의 반응이다.  
이 "보상"은 직접적인 답은 아니지만 컴퓨터에게는 간접적인 정답의 역할을 한다.  

**앞으로 강화학습을 통해 스스로 학습하는 컴퓨터를 에이전트라고 할 것이다.**  
에이전트는 환경에 대해 사전지식이 없는 상태에서 학습을 한다. 에이전트는 자신이 놓인 환경에서  
자신의 상태를 인식한 후 행동하게 된다. 그러면 환경은 에이전트에게 보상을 주고 다음 상태를 알려준다.  
이 보상을 통해 에이전트는 어떤 행동이 좋은 행동인지 간접적으로 알게된다. 
- 보상은 양수로 설정할 수도 있고 경우에 따라서 음수로 설정할 수도 있다.  
- 보상을 음수로 설정한다면 처벌이 된다.  상벌을 적적히 융합할 수 있다면 효과적인 학습이 가능하다.  

### 강화학습은 어떤 문제에 적용할까?
강화학습은 마치 사람처럼 환경과 상호작용하면서 스스로 학습되는 방식이다.  
그러므로 강화학습은 결정을 순차적으로 내려야 하는 문제에 적용한다.
현재 위치에서 행동을 한 번 선택하는 것이 아니라 계속적으로 선택해야 한다.
**하지만** 이렇게 순차적으로 결정을 내리는 문제의 해결책이 강화학습만 있는 것은 아니다.
**다이나믹 프로그래밍, 진화 알고리즘 또한 이러한 문제를 푸는데 적용할 수 있으며 강화학습이 그 한계를 극복할 수 있다.**  

### 순차적 행동 결정 문제 
100명의 학생이 있고 학생들의 수학 실력을 비교해서 수학 성적을 높일 전략을 세우려 한다면 어떻게 학생들의 수학 실력을 비교할 수 있을까?  
가장 간단한 방법은 시험을 통해 수학 실력을 수치화 하는 것이다. 만약 학생들의 수학 실력을 수치화하지 않는다면 수학 점수를 높이기 위한 전략을 세우기 어렵다.  
이와 마찬가지로 에이전트가 학습하고 발전하려면 문제를 수학적으로 표현해야 한다.   
그렇지 않으면 에이전트의 입장에서는 학습을 하거나 최적화하기 어려울 것이기 떄문이다.  

순차적으로 행동을 결정하는 문제를 정의할 때 사용하는 방법이 **MDP(Markov Decision Process)**이다.  
MDP는 순차적 행동 결정 문제를 수학적으로 정의해서 에이전트가 순차적 행동 결정 문제에 접근할 수 있게 해준다.

### 순차적 행동 결정 문제의 구성 요소
수학적으로 정의된 문제는 다음과 같은 구성을 요소를 가진다. 이 구성 요소들을 MDP라고 부른다.  

**1. 상태**  
에이전트의 상태로서 공학에서 많이 사용하는 개념이다.  
상태의 정의가 중요한데, 에이전트가 상태를 통해 상황을 판단해서 행동을 결정하기에 충분한 정보를 제공해야 한다.  
