@tf.function이란
tf.function에서는 파이썬 코드의 문장 실행 순서를 참조하여 실행 순서의 모호성을 해결한다. 이런 방법으로,
tf.fucntion에서 네트워크 연결 상태를 추적할 수 있는 작업을 정렬하면 즉시 실행 모드의 의미를 복제한다. 
최고의 성능을 얻고 어디에서나 모델을 배포할 수 있도록 하려면 프로그램 내에서 그래프를 만드는 tf.function을 사용해야 한다.
AutoGraph 덕분에 많은 양의 파이썬 코드가 tf.function으로 작동된다.
def위에 @tf.function을 붙이면 
1. @tf.function annotation을 붙이면 tf1.x 스타일로 해당 함수내의 로직이 동작(Session)
2. 따라서 상황에 따라 속도가 약간 빨라진 수 있다.
3. 다만 해당 annotation을 붙이면 값을 바로 계산해볼 수 없어서 모든 로직에 대한 프로그래밍이 끝난 뒤에 붙이는 것이 좋다.


자동미분(Automatic differentiation, Algorithmic differentiation, computaitional differentiation, auto-differentiation, 또는 줄여서 간단히 auto-diff)는
컴퓨터 프로그램에 의해서 구체화된 함수의 미분을 산술적으로 계산할 때 사용하는 기술의 집합을 말한다. 컴퓨터 프로그램에서 구체화한 함수는 아무리 복잡해보이더라도
기본적인 산술 연산 (덧셈, 뺄셈, 곱셈, 나눗셈 등)과 기본적인 함수 (지수, 로그, 싸인, 코싸인 등)의 연속적인 실행으로 이루어진다는 아이디어를 기반으로 한다.
복잡한 함수도 연쇄 법칙(Chain Rule)을 이용함으로써 합성함수를 구성하는 각 기본함수의 미분의 곱으로 나타내고 반복적으로 계산함으로써 자동으로 복잡한 함수를 정확하고 효율적으로 미분할 수 있다.

자동미분은 딥러닝에서 오차 역전파 알고리즘을 사용해서 모델을 학습할 때 유용하게 사용한다. TensorFlow는 Gradient Tapes를 이용하면 
즉시 실행 모드(eager execution mode)에서 쉽게 오차 역전파를 수행할 수 있다.

1. 즉시 실행 모드에서 자동미분을 하기 위해 Gradient tapes가 필요한 이유
예를 들어서  y = a * x 라는 방정식에서 a 와 y 가 상수(constant)이고 x 가 변수(Varialbe) 일 때, 이 방정식을 오차역전파법으로 풀어서 변수 x 를 구하고자 한다. 그러면 간단한 손실함수인 loss = abs(a * x - y)를 최소로 하는 x 를 구하면 된다. 
아래 예의 경우 8.0 = 2.0 * x 방정식 함수로 부터 변수 x 의 값을 구하고자 합니다. x 를 10.0 에서 시작해서 
abs(a * x - y) 손실함수 값을 최소로 하는 x 를 구하려면 '손실함수에 대한 x의 미분 (the gradient of the loss with respect to x)를 구해서 x 값을 미분값만큼 빼서 갱신해주어야 한다.
그런데 아래의 TensorFlow 2.x 버전에서의 Python 즉시실행모드(eager mode)에서 손실(Loss) 을 "바로 즉시 계산"(eager execution)해버려서 출력 결과를 보면 numpy=12.0 인 Tensor 상수입니다. 
여기서 자동미분을 하려고 하면 문제가 생기는데, 왜냐하면 자동미분을 하기 위해 필요한 함수와 계산 식의 연산 과정과 입력 값에 대한 정보가 즉시실행모드에서는 없기 때문이다.

2. TensorFlow에서 Gradient tapes를 이용해 자동미분하는 방법
이 문제를 해결하기 위해 TensorFlow 는 중간 연산 과정(함수, 연산)을 테이프(tape)에 차곡차곡 기록해주는 Gradient tapes 를 제공한다.
with tf.GradientTape() as tape: 로 저장할 tape을 지정해주면, 이후의 GradientTape() 문맥 아래에서의 TensorFlow의 연관 연산 코드는 tape에 저장이 된다. 
이렇게 tape에 저장된 연산 과정 (함수, 연산식) 을 가져다가 TensorFlow는 dx = tape.gradient(loss, x) 로 
후진 모드 자동 미분 (Reverse mode automaticifferentiation) 방법으로 손실에 대한 x의 미분을 계산을 한다. 
이렇게 계산한 손실에 대한 x의 미분을 역전파(backpropagation)하여 x의 값을 갱신(update)하는 작업을 반복하므로써 변수 x의 답을 찾아가는 학습을 진행한다.

# UDF for training
def train_func():
    with tf.GradientTape() as tape:
        loss = tf.math.abs(a * x - y)
        
    # calculate gradient
    dx =  tape.gradient(loss, x)
    print('x = {}, dx = {:.2f}'.format(x.numpy(), dx))

    # update x <- x - dx
    x.assign(x - dx)
    
# Run train_func() UDF repeately
for i in range(4):
    train_func()

[Out]
x = 10.0, dx = 2.00
x = 8.0, dx = 2.00
x = 6.0, dx = 2.00
x = 4.0, dx = 0.00

y = x^2 라는 함수에서 (Target) y에 대한 (Source) x의 미분(derivative of target y with respect to source x)을 
TensorFlow의 GradientTape.gradient() 메소드를 사용해서 계산해보겠다. 

x = tf.Variable(4.0)
with tf.GradientTape() as tape:
    y = x ** 2

# dy = 2x * dx
dy_dx = tape.gradient(y, x)
dy_dx.numpy()

[Out] 8.0




